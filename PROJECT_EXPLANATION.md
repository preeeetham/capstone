# Complete Project Understanding Guide
## Walmart Sales Forecasting System

---

## ðŸ“š Table of Contents
1. [Project Overview](#project-overview)
2. [What Problem Are We Solving?](#what-problem-are-we-solving)
3. [The Dataset](#the-dataset)
4. [Feature Engineering Explained](#feature-engineering-explained)
5. [Models Used and Why](#models-used-and-why)
6. [Understanding the Heatmap](#understanding-the-heatmap)
7. [How the Pipeline Works](#how-the-pipeline-works)
8. [Key Results and Metrics](#key-results-and-metrics)
9. [Presentation Talking Points](#presentation-talking-points)

---

## ðŸŽ¯ Project Overview

**What is this project?**
This is a **time series forecasting system** that predicts future weekly sales for Walmart stores and departments. It uses historical sales data from 45 stores across different departments, combined with external factors like temperature, fuel prices, and economic indicators.

**Why is this important?**
- Helps retailers **optimize inventory management**
- Enables better **staff scheduling**
- Improves **demand planning** and **supply chain** efficiency
- Reduces costs by predicting slow/fast periods

---

## ðŸ¤” What Problem Are We Solving?

### The Business Problem
Walmart needs to predict how much each department in each store will sell next week. Accurate predictions help them:
- **Stock the right amount** of products (not too much = waste, not too little = lost sales)
- **Schedule employees** appropriately (more staff during busy periods)
- **Plan promotions** strategically

### The Technical Challenge
Sales forecasting is difficult because:
1. **Seasonality**: Sales vary by time of year (holidays, summer, etc.)
2. **Trends**: Long-term patterns (growing or declining sales)
3. **External factors**: Weather, economy, fuel prices affect shopping behavior
4. **Store differences**: Different stores and departments behave differently

---

## ðŸ“Š The Dataset

### Source
- **Real data from Kaggle**: "Walmart Store Sales Forecasting"
- Downloaded automatically using `kagglehub` library

### Dataset Size
- **421,570 training samples** (historical data)
- **115,064 test samples** (future predictions)
- **45 stores**
- **99 departments**
- **Time period**: February 2010 to October 2012

### What's in the Data?

#### Core Variables
| Variable | What It Means | Example |
|----------|---------------|---------|
| **Store** | Store number (1-45) | Store #12 |
| **Dept** | Department number (1-99) | Electronics = Dept 72 |
| **Date** | Week ending date | 2012-04-13 |
| **Weekly_Sales** | Sales for that week (TARGET) | $24,924.50 |

#### External Variables (Things That Affect Sales)
| Variable | What It Means | Why It Matters |
|----------|---------------|----------------|
| **Temperature** | Average temp that week | Hot = more ice cream; Cold = more heaters |
| **Fuel_Price** | Gas price per gallon | High gas prices = people shop less |
| **CPI** | Consumer Price Index | Measures inflation/economy |
| **Unemployment** | Unemployment rate | High unemployment = less spending |
| **IsHoliday** | Holiday week or not | Holidays = higher sales |
| **MarkDown1-5** | Promotional discounts | Markdowns = temporary price cuts |

#### Store Characteristics
| Variable | What It Means | Impact |
|----------|---------------|--------|
| **Type** | Store category (A, B, C) | Type A = largest stores |
| **Size** | Store size in sq ft | Bigger stores = more sales |

---

## ðŸ”§ Feature Engineering Explained

**What is Feature Engineering?**
Creating new variables from existing data to help models make better predictions.

### 1. **Lag Features** (Looking Back in Time)
These are **previous week's values**.

**Example**: If predicting sales for Week 10:
- `Weekly_Sales_lag_1` = Sales from Week 9 (last week)
- `Weekly_Sales_lag_2` = Sales from Week 8 (2 weeks ago)
- `Weekly_Sales_lag_4` = Sales from Week 6 (4 weeks ago)
- `Weekly_Sales_lag_12` = Sales from 12 weeks ago (3 months)

**Why useful?** Sales often follow patterns - if you sold $10,000 last week, you'll probably sell something similar this week.

**Our lag features (12 total)**:
- Weekly_Sales: lags 1, 2, 4, 12
- Temperature: lags 1, 4
- Fuel_Price: lags 1, 4
- CPI: lags 1, 4
- Unemployment: lags 1, 4

### 2. **Rolling Statistics** (Moving Windows)
Calculate statistics over the **past N weeks**.

**Example**: 4-week rolling mean:
```
Week 1: $5,000
Week 2: $6,000
Week 3: $7,000
Week 4: $8,000
4-week mean = ($5,000 + $6,000 + $7,000 + $8,000) / 4 = $6,500
```

**Our rolling features (12 total)**:
- **4-week window**: mean, std (volatility), max, min
- **8-week window**: mean, std, max, min
- **12-week window**: mean, std, max, min

**Why useful?** Captures trends and smooths out random noise.

### 3. **Calendar Features** (Time Patterns)
These capture **seasonal patterns**.

**Temporal features**:
- `week` = week number (1-52)
- `month` = month (1-12)
- `quarter` = Q1, Q2, Q3, Q4
- `year` = 2010, 2011, 2012
- `day_of_week` = Monday-Sunday
- `day_of_month` = 1-31

**Cyclical encoding** (sin/cos):
Why? December (month 12) is close to January (month 1), but numerically 12 is far from 1. Sin/cos encoding fixes this:
- `month_sin`, `month_cos`
- `week_sin`, `week_cos`
- `day_of_week_sin`, `day_of_week_cos`

**Why useful?** Sales have strong weekly/monthly patterns (weekends, holidays, seasons).

### 4. **External Features** (Economic Indicators)
Already in the dataset, but we also create **lagged versions** because economic changes take time to affect shopping behavior.

**Total Features Created: 49**

---

## ðŸ¤– Models Used and Why

We implemented **5 different models** from simple to complex.

### 1. **Naive Forecast** (Baseline)
- **Type**: Statistical baseline
- **How it works**: Predicts next week's sales = last week's sales
- **Formula**: Prediction = Previous week's actual sales
- **When to use**: Quick benchmark, simple products with stable demand
- **Our results**: RMSE = 32,047.87 (worst performer)
- **Example**: If Store 1, Dept 5 sold $10,000 last week, predict $10,000 for next week

### 2. **Moving Average** (Baseline)
- **Type**: Statistical baseline
- **How it works**: Average of past 4 weeks
- **Formula**: Prediction = (Week-1 + Week-2 + Week-3 + Week-4) / 4
- **When to use**: Smooth out random fluctuations, stable trends
- **Our results**: RMSE = 31,094.09
- **Better than**: Naive (slightly)

### 3. **LightGBM** (Best Model! ðŸ†)
- **Type**: Gradient Boosting Decision Trees (Machine Learning)
- **Company**: Microsoft
- **How it works**: 
  - Builds many decision trees sequentially
  - Each tree corrects errors of previous trees
  - "Light" = optimized for speed and memory
- **Strengths**:
  - Handles large datasets well (421K rows)
  - Automatically handles missing data
  - Captures complex patterns and interactions
  - Fast training and prediction
- **Parameters that affect it**:
  - `num_leaves`: How complex each tree is (31)
  - `learning_rate`: How fast it learns (0.05 = slow & accurate)
  - `feature_fraction`: % of features per tree (90%)
  - `num_boost_round`: How many trees to build (stopped at 336)
- **Our results**: RMSE = 2,880.83 âœ… BEST!
- **Why it won**: Great at finding patterns in lag/rolling features

### 4. **XGBoost** (Second Best)
- **Type**: Extreme Gradient Boosting (Machine Learning)
- **Company**: Open source (DMLC)
- **How it works**: Similar to LightGBM but different optimization
  - Also builds trees sequentially
  - Uses "extreme" regularization to prevent overfitting
- **Strengths**:
  - Very accurate and robust
  - Handles outliers well
  - Built-in cross-validation
- **Parameters that affect it**:
  - `max_depth`: Tree depth (6 levels)
  - `learning_rate`: Speed of learning (0.05)
  - `subsample`: % of data per tree (80%)
  - `colsample_bytree`: % of features per tree (80%)
- **Our results**: RMSE = 3,022.41
- **Why second**: Very close to LightGBM, slightly slower

### 5. **LSTM** (Deep Learning)
- **Type**: Long Short-Term Memory Neural Network
- **Field**: Deep Learning / AI
- **How it works**:
  - Neural network designed for **sequences** (time series)
  - Has "memory cells" that remember long-term patterns
  - Processes data in **sequences** (e.g., past 12 weeks â†’ predict week 13)
- **Architecture**:
  - Input: Sequence of 12 weeks of features
  - Hidden layers: 2 LSTM layers with 50 units each
  - Output: Single prediction (next week's sales)
- **Strengths**:
  - Excellent for complex temporal patterns
  - Can capture long-term dependencies
  - Learns automatically from data
- **Parameters that affect it**:
  - `sequence_length`: How many past weeks to look at (12)
  - `units`: Neurons per layer (50)
  - `epochs`: Training iterations (50)
  - `batch_size`: Samples per training step (32)
- **Our results**: RMSE = 27,030.13 (needs TensorFlow installed)
- **Note**: Currently using fallback because TensorFlow not installed in environment

---

## ðŸ“Š Understanding the Heatmap

### What is a Heatmap?
A **visual representation of correlations** between variables. It shows which features move together.

### What is Correlation?
**Correlation** measures how two variables relate (range: -1 to +1):
- **+1.0** = Perfect positive correlation (when A goes up, B goes up)
- **0.0** = No correlation (A and B are independent)
- **-1.0** = Perfect negative correlation (when A goes up, B goes down)

### Reading the Heatmap

**Color Meaning**:
- ðŸ”´ **Red/Dark red** = Strong positive correlation (close to +1)
- âšª **White** = No correlation (close to 0)
- ðŸ”µ **Blue** = Negative correlation (close to -1)

**Key Findings from Our Heatmap**:

1. **Weekly_Sales vs Weekly_Sales_lag_1 = 0.95** ðŸ”¥
   - Last week's sales strongly predict this week's sales
   - Makes sense: stores have consistent patterns

2. **Weekly_Sales vs Rolling_mean_4 = 0.95** ðŸ”¥
   - 4-week average is an excellent predictor
   - Smooths out noise, captures trends

3. **Weekly_Sales vs Size = 0.24**
   - Bigger stores have somewhat higher sales
   - But not as strong as time-based features

4. **Temperature vs Fuel_Price â‰ˆ 0.30**
   - Slight positive correlation
   - Summer = driving more = higher fuel demand

5. **MarkDown features have weak correlations**
   - Lots of missing data (64-74% missing)
   - Promotions are sparse and inconsistent

### Why This Matters
- Features with **high correlation to Weekly_Sales** are most important
- Models use these to make predictions
- Helps us understand which factors actually drive sales

---

## âš™ï¸ How the Pipeline Works

### Step-by-Step Execution

```
1. DATA LOADING
   â†“
   - Download dataset from Kaggle (421,570 rows)
   - Parse dates, merge store/features data
   - Clean and sort by Store/Dept/Date

2. EXPLORATORY DATA ANALYSIS (EDA)
   â†“
   - Check missing values (MarkDown features 64-74% missing)
   - Detect outliers (8.43% of sales are outliers)
   - Visualize trends, seasonality, holidays
   - Generate plots: sales_trends, holiday_analysis, external_variables

3. FEATURE ENGINEERING
   â†“
   - Create 12 lag features (t-1, t-2, t-4, t-12)
   - Calculate 12 rolling statistics (mean/std/max/min over 4/8/12 weeks)
   - Add 8 calendar features (week, month, cyclical encoding)
   - Total: 49 features created

4. TRAIN/VALIDATION SPLIT
   â†“
   - Time-based split: 80% train, 20% validation
   - Train: 2010-02-05 to 2012-04-13
   - Validation: 2012-04-13 to 2012-10-26
   - NO RANDOM SHUFFLING (respects time order)

5. MODEL TRAINING
   â†“
   BASELINES:
   - Naive: Store last value per Store-Dept
   - Moving Average: Calculate 4-week average
   
   MACHINE LEARNING:
   - LightGBM: Train 336 trees with early stopping
   - XGBoost: Train with regularization
   - LSTM: Train neural network on sequences

6. PREDICTION
   â†“
   - Each model predicts validation period (84,314 samples)
   - Generate predictions for all Store-Dept combinations
   - Predictions are continuous values (e.g., $15,234.67)

7. EVALUATION
   â†“
   - Calculate RMSE, MAE, MAPE for each model
   - Compare predictions vs actual sales
   - Identify best model (LightGBM)

8. VISUALIZATION
   â†“
   - Plot predicted vs actual for each model
   - Create comparison charts
   - Generate correlation heatmap
   - Save all results to results/ folder
```

### File Structure
```
capstone/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ data_loader.py           # Kaggle download & preprocessing
â”‚   â”œâ”€â”€ eda.py                   # Exploratory analysis
â”‚   â”œâ”€â”€ feature_engineering.py   # Create features
â”‚   â”œâ”€â”€ models.py                # All 5 models implemented
â”‚   â”œâ”€â”€ evaluation.py            # Metrics & plotting
â”‚   â””â”€â”€ utils.py                 # Helper functions
â”œâ”€â”€ notebooks/                    # Jupyter notebook
â”‚   â””â”€â”€ sales_forecasting_pipeline.ipynb
â”œâ”€â”€ results/                      # Generated outputs
â”‚   â”œâ”€â”€ correlation_heatmap.png
â”‚   â”œâ”€â”€ all_predictions.png
â”‚   â”œâ”€â”€ lightgbm_predictions.png
â”‚   â””â”€â”€ model_comparison.csv
â”œâ”€â”€ run_pipeline.py              # Main execution script
â”œâ”€â”€ run_pipeline_fast.py         # Fast version (skip slow models)
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # Documentation
```

---

## ðŸ“ˆ Key Results and Metrics

### Evaluation Metrics Explained

#### 1. **RMSE (Root Mean Squared Error)**
- **What it is**: Average prediction error in dollars
- **Formula**: âˆš(mean((actual - predicted)Â²))
- **Lower is better**
- **Interpretation**: "On average, predictions are off by $X"
- **Why squared?**: Penalizes large errors more heavily
- **Example**: RMSE = 2,880.83 means predictions are off by ~$2,881 on average

#### 2. **MAE (Mean Absolute Error)**
- **What it is**: Average absolute error in dollars
- **Formula**: mean(|actual - predicted|)
- **Lower is better**
- **Interpretation**: "Typical prediction error is $X"
- **Easier to understand** than RMSE (no squaring)
- **Example**: MAE = 1,393.87 means typical error is ~$1,394

#### 3. **MAPE (Mean Absolute Percentage Error)**
- **What it is**: Average error as a percentage
- **Formula**: mean(|actual - predicted| / actual) Ã— 100
- **Lower is better**
- **Interpretation**: "Predictions are off by X%"
- **Problem**: Infinity when actual = 0 (happens with low sales)
- **Our values are high** because of division by very small sales numbers

### Model Comparison Table

| Model | RMSE ($) | Improvement vs Baseline | When to Use |
|-------|----------|------------------------|-------------|
| **LightGBM** | 2,880.83 | **91% better** âœ… | Production, large datasets, need speed |
| **XGBoost** | 3,022.41 | 91% better | Production, need robustness |
| **LSTM** | 27,030.13* | - | Complex patterns, have GPU, lots of data |
| **Moving Avg** | 31,094.09 | Baseline | Quick estimates, stable products |
| **Naive** | 32,047.87 | Baseline | Simplest benchmark |

*LSTM currently on fallback (TensorFlow not installed)

### Why LightGBM Won

1. **Lag features are highly correlated** (0.95) â†’ LightGBM captures these perfectly
2. **Rolling statistics** provide smooth trends â†’ Trees handle this well
3. **Missing data** in MarkDowns â†’ LightGBM handles missing values natively
4. **Large dataset** (421K rows) â†’ LightGBM is optimized for scale
5. **Fast training** â†’ Early stopping at iteration 336 (out of 1000)

---

## ðŸŽ¤ Presentation Talking Points

### Opening (30 seconds)
> "I built a sales forecasting system that predicts weekly sales for Walmart stores with 91% better accuracy than baseline methods. The system uses real Kaggle data with 421,000 historical records across 45 stores and implements 5 different machine learning models."

### The Problem (1 minute)
> "Retailers face a challenge: how much inventory should each store order? Too much = wasted money. Too little = lost sales. My system solves this by predicting next week's sales using historical patterns, economic indicators, and advanced machine learning."

### The Data (1 minute)
> "I used real Walmart data from Kaggle with:
> - 421,000 training samples across 45 stores and 99 departments
> - 2.5 years of historical data (2010-2012)
> - External factors like temperature, fuel prices, unemployment, and holiday markers
> - The target: Weekly_Sales - what we're trying to predict"

### Feature Engineering (2 minutes)
> "I didn't just use raw data - I engineered 49 features:
> 
> **Lag features**: Last week's sales, 2 weeks ago, 4 weeks ago, 12 weeks ago. Why? Because sales follow patterns - if you sold $10K last week, you'll probably sell similar amounts this week.
> 
> **Rolling statistics**: 4-week, 8-week, and 12-week averages. This smooths out random noise and captures trends.
> 
> **Calendar features**: Week, month, quarter, year - but also sine/cosine encoding because December (month 12) is actually close to January (month 1).
> 
> The correlation heatmap shows lag features have 0.95+ correlation with sales - that's why they're so powerful."

### The Models (3 minutes)
> "I implemented 5 models from simple to complex:
> 
> **Baselines** (Naive & Moving Average):
> - Simple benchmarks to beat
> - RMSE ~31,000-32,000
> 
> **LightGBM** (Winner! ðŸ†):
> - Gradient boosting with 336 decision trees
> - Each tree learns from previous trees' mistakes
> - RMSE: 2,880 - that's 91% better than baseline!
> - Why it won: Perfectly captures lag features and handles missing data
> 
> **XGBoost**:
> - Similar to LightGBM, slightly different algorithm
> - RMSE: 3,022 - almost as good
> - More robust to outliers
> 
> **LSTM Neural Network**:
> - Deep learning model with memory cells
> - Designed for sequential data (time series)
> - Processes sequences of 12 weeks to predict week 13
> - Can capture complex long-term patterns"

### The Heatmap (1 minute)
> "The correlation heatmap reveals what drives sales:
> - Last week's sales: 0.95 correlation - strongest predictor
> - 4-week rolling average: 0.95 correlation
> - Store size: 0.24 correlation - moderate effect
> - MarkDowns: weak correlation - too much missing data
> 
> This tells us temporal patterns matter more than anything else."

### Results & Impact (1 minute)
> "**Best model: LightGBM with RMSE of $2,880**
> 
> What does this mean?
> - Predictions are typically off by ~$2,880 per store-department per week
> - For a store selling $15,000/week, that's 19% error
> - 91% improvement over naive forecasting
> 
> **Business impact:**
> - Better inventory planning â†’ reduce waste
> - Optimized staffing â†’ lower labor costs
> - Data-driven decisions â†’ increase profitability"

### Technical Highlights (1 minute)
> "**What makes this project strong:**
> 1. Real Kaggle data - not toy dataset
> 2. Proper time-based train/test split - no data leakage
> 3. Comprehensive feature engineering - 49 features
> 4. Multiple model comparison - 5 different approaches
> 5. Proper evaluation - RMSE, MAE, MAPE metrics
> 6. Production-ready code - modular, documented, reproducible"

### Closing (30 seconds)
> "This system demonstrates end-to-end machine learning: data collection, cleaning, feature engineering, model training, evaluation, and visualization. The 91% improvement over baseline proves that machine learning adds real business value to forecasting problems."

---

## ðŸŽ¯ Questions Your Mentor Might Ask

### Q: "Why did you choose these specific models?"
**A**: "I implemented a range from simple to complex:
- Baselines (Naive, MA) for benchmarking
- Tree-based models (LightGBM, XGBoost) because they handle tabular data excellently and captured the high correlation in lag features
- LSTM for deep learning approach to sequential patterns
This gives a comprehensive comparison across different techniques."

### Q: "Why is LightGBM better than LSTM?"
**A**: "Two main reasons:
1. Our features (lags, rolling stats) already capture temporal patterns explicitly - trees can learn from these directly
2. LSTMs need very large datasets and careful tuning to outperform - and work best with raw sequences, not heavily engineered features
For tabular time series with strong lag correlations, tree-based models typically win."

### Q: "What's the business value?"
**A**: "With 91% improvement over baseline:
- Reduce overstock waste (millions in savings)
- Prevent stockouts (capture lost sales)
- Optimize labor scheduling (right staff at right time)
- Even 1% improvement in forecast accuracy can save retailers millions annually"

### Q: "How would you improve this further?"
**A**: "Several approaches:
1. Hyperparameter tuning (grid search, Bayesian optimization)
2. Ensemble methods (combine LightGBM + XGBoost)
3. More external features (weather, local events, competitor pricing)
4. Per-store models (train separate models for different store types)
5. Install TensorFlow and properly tune LSTM
6. Handle missing MarkDown data better (imputation strategies)"

### Q: "What about the missing data?"
**A**: "MarkDown features have 64-74% missing values. I kept them because:
1. When present, they indicate promotional periods
2. LightGBM/XGBoost handle missing data natively
3. Missing itself is informative (no promotion = missing)
Future improvement: impute with 0 or create 'has_promotion' binary flag."

### Q: "How do you prevent overfitting?"
**A**: "Multiple strategies:
1. Time-based split (no data leakage)
2. Early stopping in LightGBM (stopped at 336/1000 iterations)
3. Regularization in XGBoost (subsample, colsample)
4. Validation set for monitoring
5. Cross-validation would be next step (time-series CV)"

### Q: "Can this run in production?"
**A**: "Yes, with minor changes:
1. Code is modular and documented
2. LightGBM is fast (milliseconds per prediction)
3. Would add: API wrapper, model versioning, monitoring, retraining pipeline
4. Deploy: Docker container, cloud service (AWS SageMaker, Azure ML)
5. Already implements best practices (time-based split, proper evaluation)"

---

## ðŸ“š Key Terms Glossary

- **Time Series**: Data collected over time (sales by week)
- **Forecasting**: Predicting future values
- **Feature**: Input variable (column in data)
- **Target**: What we're predicting (Weekly_Sales)
- **Train/Test Split**: Dividing data for training and evaluation
- **Overfitting**: Model memorizes training data, fails on new data
- **RMSE**: Root Mean Squared Error - average prediction error
- **Correlation**: How two variables relate (-1 to +1)
- **Gradient Boosting**: Building trees sequentially to correct errors
- **Neural Network**: AI model inspired by brain neurons
- **Sequence**: Ordered data (e.g., past 12 weeks)
- **Regularization**: Preventing overfitting by constraining complexity
- **Hyperparameter**: Setting you tune (learning rate, tree depth)
- **Early Stopping**: Stop training when validation performance plateaus

---

## âœ… Checklist for Presentation

- [ ] Understand the business problem (inventory planning)
- [ ] Explain the dataset (421K rows, 45 stores, 2.5 years)
- [ ] Describe feature engineering (lag, rolling, calendar)
- [ ] Know why lag features matter (0.95 correlation)
- [ ] Explain each model's purpose
- [ ] Know why LightGBM won (captures lag features perfectly)
- [ ] Interpret the heatmap (red=correlation, white=none)
- [ ] Understand metrics (RMSE, MAE, MAPE)
- [ ] Articulate business impact (91% improvement)
- [ ] Be ready for improvement questions

---

**Good luck with your presentation! You've built a solid, production-quality forecasting system! ðŸš€**
